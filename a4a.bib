%% "A's for All" (CBT/CELL) project bibliography

%% please add ALL relevant papers, presentations, posters, etc, 
%% in bibtex format, in the correct section!

%%% refereed full papers (conference or journal)
                  
@InProceedings{a4a-sigcse2023-paper,
  author =       {Dan Garcia and Armando Fox and Craig Zilles and Matthew West and Mariana Silva and Neal Terrell and Solomon Russell and Edwin Ambrosio and Fuzail Shakir},
  title =        {A's For All (As Time and Interest Allow)},
  booktitle = {{SIGCSE} 2023},
  address = {Toronto, ON, CANADA},
  month =     3,
  year =      2023}

                  
@InProceedings{parsons-chi2021,
  author =       {Nathaniel Weinman and Armando Fox and Marti A. Hearst},
  title =        {Improving Instruction of Programming Patterns With Faded {P}arsons Problems},
  booktitle = {2021 {ACM} {CHI} Virtual Conference on Human Factors in Computing Systems ({CHI} 2021)},
  year =      2021,
  month =     5,
  address =   {Yokohama, Japan (online virtual conference)}}


@InProceedings{rspec-faded-parsons,
  author =       {Nelson Lojo and Armando Fox},
  title =        {Teaching Test-Writing As a Variably-Scaffolded Programming Pattern},
  booktitle = {27th Annual Conference on Innovation and Technology in Computer Science Education ({ITiCSE} 2022)},
  year =      2022,
  month =     7,
  address =   {Dublin, Ireland},
  note =      {30\% accept rate}}
                  
@inproceedings{async-exams-sigcse2022-paper,
author = {McMahon, Connor and Yao, Bojin and Yokota, Justin and Garcia, Dan},
title = {Lessons Learned from Asynchronous Online Assessment Formats in CS0 and CS3},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3478431.3499386},
doi = {10.1145/3478431.3499386},
abstract = {This paper provides an experience report for the asynchronous online assessments in two classes in Fall 2020: CS0 and CS3. The two courses shared many structural similarities; both were taught by the same instructor, with three exams delivered asynchronously using the same submission and question randomization software. They differed in proctoring rules, time allowances, student feedback, and lessons learned. In both classes, the first exam followed the same format, with students being given unlimited time over a 24-hour period to take the exam; later exams diverged due to different preferences among students in each class. CS0 historically had extremely low rates of academic dishonesty, so no formal proctoring was mandated. Thankfully we did not find evidence of academic impropriety at a higher rate than in-person semesters. Students universally loved the untimed format due to the flexibility that it provided, which led us to increasing the time window in which students could take the exam. Cheating was more prevalent within CS3 during in-person semesters, so it was not appropriate to have unproctored exams. Instead, we required students to submit a recording of themselves taking the exam. Even with these proctoring policies in place, students still found ways to cheat. Additionally, many students reported that the unlimited time increased their stress, leading us to restrict the time policies as the semester progressed. Our takeaway is that multi-day unproctored randomized exams are effective in a student population with high honor-code adherence, but finding the right policy for a high-stress major course remains elusive.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {640–646},
numpages = {7},
keywords = {randomized exams, take-home exams, asynchronous online exams},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}


%%% non-refereed pubs (tech reports, ArXiv, etc)
@misc{https://doi.org/10.48550/arxiv.2303.05995,
  doi = {10.48550/ARXIV.2303.05995},
  url = {https://arxiv.org/abs/2303.05995},
  author = {Durán, Amador and Fernández, Pablo and Bernárdez, Beatriz and Weinman, Nathaniel and Akalın, Aslıhan and Fox, Armando},
  keywords = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Exploring Gender Bias in Remote Pair Programming among Software Engineering Students: The twincode Original Study and First External Replication},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}



@techreport{fpp-element-techreport,
    Author = {Caraco, Logan and Weinman, Nate and Ko, Stanley and Fox, Armando},
    Title = {Automatically Converting Code-Writing Exercises to Variably-Scaffolded Parsons Problems},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2022},
    Month = 6,
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-173.html},
    Number = {UCB/EECS-2022-173},
    Abstract = {We present a system for automatically converting existing code-writing exercises in Python into Faded Parsons Problems (FPPs) that students solve interactively in a Web browser. FPPs were introduced by Weinman et al. as a novel exercise interface for teaching programming patterns or idioms. Like original Parsons Problems, FPPs ask students to arrange lines of code to reconstruct a correct solution. Unlike original Parsons Problems, FPPs can also ask students to fill in some blanks in the provided lines of code, in addition to ordering the lines. In our system, which extends the open-source PrairieLearn platform, the student uses a Web browser to fill in blanks, reorder the lines of code, or both. The student can check their work at any time using an autograder that runs the student-submitted code against instructor-provided test cases; feedback to the student can be as fine-grained as the test cases allow. Converting existing code-writing exercises to FPPs is nearly automatic. Manually changing the amount of scaffolding in the FPP is easy and amenable to future automation. Instructors can thereby take advantage of initial study findings that FPPs outperform code-writing and code-tracing exercises as a way of teaching programming patterns, and how FPPs improve overall code-writing ability at a level comparable to code-writing exercises but are preferred by students.}
}

@inproceedings{twincode_ESEM2021,
  author    = {A. Durán and P. Fernández and B. Bernárdez and N. Weinman and A. Akal{\i}n and A. Fox},
  title     = {Gender Bias in Remote Pair Programming among Software Engineering Students: The twincode Exploratory Study},
  booktitle = {Proceedings of ESEM 2021---Registered Report Track},
  year      = 2021,
  url       = {https://arxiv.org/abs/2110.01962},
  annote = {presentation at https://youtu.be/9GxBfwnOwDQ?t=425}
}




%%% invited presentations (conference, private audiences, etc)

@misc{nextprof-nexus-2022-fostering-effective-learning,
  author       = "Dan Garcia",
  title        = "Fostering Effective Learning",
  howpublished = "Keynote for NEXTProf NEXUS Conference in Berkeley CA",
  month        = "9",
  year         = "2022",
}

@misc{uc-regents-2022,
  author       = "Dan Garcia and Armando Fox",
  title        = "Innovations in Assessment and Grading at the University of California",
  howpublished = "Virtual presentation for the 2022-03-16 Academic and Student Affairs Committee of the University of California Board of Regents",
  month        = "3",
  year         = "2022",
}

%%% posters, WIPs, other short/non-archival publications, panels, workshops, demos, etc

@inproceedings{LatS-Demo-SS-for-a4a,
author = {Garcia, Dan and McMahon, Connor and Garcia, Yuan and West, Matthew and Zilles, Craig},
title = {Software Support for "A's for All"},
year = {2022},
isbn = {9781450391580},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3491140.3528260},
doi = {10.1145/3491140.3528260},
abstract = {The SIGCSE-MEMBERS mailing list of the ACM Special Interest Group in Computer Science Education is the main forum for educators worldwide to discuss computing education research, pedagogy, and curriculum. In early 2022 it was abuzz with several connected movements: growth mindset, proficiency (aka mastery) learning, grading for equity, and specifications grading. Each of these is an important step toward the Holy Grail: A's for All (as time and interest allow); the "A" line doesn't move, but every student should be given an opportunity to achieve proficiency and earn it, as long as they are willing to put in the time and effort it might take. The mantra is not "fixed time, variable learning", but "fixed learning, variable time".Many of us have been associated with this movement for quite some time, and it is only now that the software, curriculum, and policies have evolved to the state that we are ready to demonstrate how it works "at scale". The goal of this demo is to highlight the software infrastructure required for educators to achieve it in their courses and institutions; this is associated with the workshop Achieving "A's for All (as time and interest allow)".},
booktitle = {Proceedings of the Ninth ACM Conference on Learning @ Scale},
pages = {475–476},
numpages = {2},
keywords = {communities of practice, scalable, pedagogy, assessment},
series = {L@S '22}
}

@inproceedings{LatS-Workshop-Achieving-a4a,
author = {Garcia, Dan and McMahon, Connor and Garcia, Yuan and West, Matthew and Zilles, Craig},
title = {Achieving "A's for All (as Time and Interest Allow)"},
year = {2022},
isbn = {9781450391580},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3491140.3528289},
doi = {10.1145/3491140.3528289},
abstract = {The SIGCSE-MEMBERS mailing list of the ACM Special Interest Group in Computer Science Education is the main forum for educators worldwide to discuss computing education research, pedagogy, and curriculum. In early 2022 it was abuzz with several connected movements: growth mindset, proficiency (aka mastery) learning, grading for equity, and specifications grading. Each of these is an important step toward the Holy Grail: A's for All (as time and interest allow); the "A" line doesn't move, but every student should be given an opportunity to achieve proficiency and earn it, as long as they are willing to put in the time and effort it might take. The mantra is not "fixed time, variable learning", but "fixed learning, variable time". The goal of this full-day workshop is to provide educators and administrators with tools to achieve it in their courses and institutions.One useful analogy is that enrolling in a course is like taking a trip by bus: there is one speed for everyone, it lasts 10 or 15 weeks, and at the end of the trip a calculation is made regarding what percentage of the pile of points has been earned, and an A-F grade is assigned. What happens when someone needs (or wants) to travel slower, and as a result will get off the bus? Do they wait a whole term for the next bus? Can they work on their own? Do they get on the "local" bus that goes slower, perhaps with a supportive cohort who also needed to go slower? This is typically not allowed by our assembly-line model of learning, yet there are many ways that an institution can support these students, which we will address.From ten miles up, what are all the elements that have to align to allow it to happen? It helps to have a leader who is passionate about equity, supported by their administration, and willing to put in a significant amount of work up front. Many in this Learning@Scale community who have already moved their traditional University class to a MOOC will find those efforts bearing fruit here. If the course is the first through the gate to adopt this model, there may be socialization among colleagues that is needed.},
booktitle = {Proceedings of the Ninth ACM Conference on Learning @ Scale},
pages = {255–258},
numpages = {4},
keywords = {communities of practice, assessment, pedagogy, scalable},
location = {New York City, NY, USA},
series = {L@S '22}
}

@InProceedings{a4a-sigcse2023-panel,
  author =       {Dan Garcia and Maria Camarena and Kevin Lin and Jill Westerlund},
  title =        {Equitable Grading Best Practices},
  booktitle = {{SIGCSE} 2023},
  month =     3,
  location = {Toronto, ON, CANADA},
  year =      2023}


@InProceedings{a4a-sigcse2023-islagiaat-panel,
  author =       {Dan Garcia and Jim Huggins and Lauren Bricker and Adam Gaweda and David J. Malan and Joël Porquet-Lupine and Kristin Stephens-Martinez},
  title =        {It Seemed Like a Good Idea at the Time (“Let Me Help You with That” edition)},
  booktitle = {{SIGCSE} 2023},
  month =     3,
  location = {Toronto, ON, CANADA},
  year =      2023}

@InProceedings{a4a-sigcse2023-poster,
  author =       {Ruiwei Xiao and Eduardo Huerta-Mercado and Dan Garcia},
  title =        {Detecting Cheating in Online Take-Home Exams with Randomized Questions},
  booktitle = {{SIGCSE} 2023},
  month =     3,
  location = {Toronto, ON, CANADA},
  year =      2023}

@InProceedings{a4a-sigcse2023-workshop,
  author =       {Dan Garcia and Connor McMahon and Yuan Garcia and Craig Zilles and Matthew West and Mariana Silva and Solomon Russell and Edwin Ambrosio and Neal Terrell},
  title =        {Actually Achieving “A's For All” (as time and interest allow)},
  booktitle = {{SIGCSE} 2023},
  month =     3,
  location = {Toronto, ON, CANADA},
  year =      2023}

@inproceedings{Weinman2020,
  doi = {10.1145/3328778.3372639},
  url = {https://doi.org/10.1145/3328778.3372639},
  year = {2020},
  month = feb,
  publisher = {{ACM}},
  address = {New York, NY, USA},
  author = {Nathaniel Weinman and Armando Fox and Marti Hearst},
  title = {Exploring Challenging Variations of Parsons Problems},
  pages = {1349},
  numpages = {1},
  booktitle = {Proceedings of the 51st {ACM} Technical Symposium on Computer Science Education}
}

@inproceedings{sigcse-22-programming-IDEs-into-CBTS-poster,
author = {Yagubyan, Abel and Garcia, Dan},
title = {Seamless Embedding of Programming IDEs into Computer-Based Testing Software},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3478432.3499122},
doi = {10.1145/3478432.3499122},
abstract = {Interest in computer-based assessment has increased in recent years, most certainly due to a shift to online learning due to the COVID pandemic. Instructors are creating questiongenerators for Computer Science classes on PrairieLearn (PL), an open-source platform developed at the University of Illinois at Urbana-Champaign PrairieLearn. The software generates differentvariants of each question to students through randomization. The challenge up to now has been that automatically graded coding problems in RISC-V or Snap!, some of the significant languages used in undergraduate Computer Science courses at our university, weren't possible to do within the software. Thequestion could be displayed, but then the student would have to load their favorite integrated development environment (IDE), code it up, and thenreturn to PL to upload their solution. This poster discusses our approach to embedding interactive development environments for Venus (RISC-V) and Snap! directly into PrairieLearn, so students never have to leave the browser tab!},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1168},
numpages = {1},
keywords = {mastery learning, computer-based testing},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{sigcse-22-improved-testing-PQGs-poster,
author = {Shah, Aayush and Lee, Alan and Chi, Chris and Xiao, Ruiwei and Sukumar, Pranav and Villalobos, Jesus and Garcia, Dan},
title = {Improved Testing of PrairieLearn Question Generators},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3478432.3499113},
doi = {10.1145/3478432.3499113},
abstract = {With many institutions forced online due to the pandemic, assessments became a challenge for many educators. Take-home exams provided the flexibility required for varied student needs (and time zones), but they were vulnerable to cheating. In response, many turned to tools that could present a different exam for every student. PrairieLearn is a feature-rich open-source package that allows educators to author randomized Question Generators; we have been using the tool extensively for the last two years, and it has a fast-growing educator user base. One of the first issues we noticed with the system was that the only way to quality assure (QA) a question was to click the new variant button, which would spin whatever internal random number generators were used again to produce a new question. Sometimes it was the same one you had just seen, and other times it would never seem to "hit'' on the variant you were looking to debug. This poster describes our team's work to solve this problem through the design of an API that would allow a question to declare how many total variants it had, and be asked to render variant i. The user interface could then be extended to list what variant the QA team was viewing out of the total (e.g., 7/50), and a next, previous and go to a particular variant buttons would allow for the team to easily QA all variants.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1165},
numpages = {1},
keywords = {quality assurance, computer-based testing},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{sigcse-21-categorizing-variants-for-QGs-in-CBA-poster,
author = {Yao, Bojin and Liao, Qitian and McMahon, Connor and Garcia, Daniel D.},
title = {Formal Categorization of Variants for Question Generators in Computer-Based Assessments},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3408877.3439683},
doi = {10.1145/3408877.3439683},
abstract = {With many Universities and Institutions moving toward online learning during the pandemic, content delivery and assessment become a new challenge for many educators. It comes as no surprise that many tools aimed at delivering computer-based assessments have become popular and are fields of active research. Members of the ACE Lab at UC Berkeley are creating question generators for computer science classes on a platform called PrairieLearn, to generate different randomized variants of questions. This poster discusses our team's approach to question variant categorization, which we believe would be useful metadata for other educators reusing our questions or generating their own.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1244},
numpages = {1},
keywords = {prairielearn, metadata, question generators, online learning},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{sigcse-22-islagiatt-no-late-penalties-panel,
author = {Garcia, Dan and Huggins, Jim and Alvarado, Christine and Gestwicki, Paul and Gunawardena, Andy and Hong, Victoria and Spertus, Ellen},
title = {It Seemed Like a Good Idea at the Time (COVID-19 Edition)},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3478432.3499250},
doi = {10.1145/3478432.3499250},
abstract = {Conference presentations usually focus on successful innovations: new ideas that yield significant improvements to current practice. Yet we often learn more from failure than from success. In this panel, we present five case studies of "good ideas" for improving CS education (most related to the COVID-19 pandemic) that didn't go as planned. Each contributor will describe their "good idea", the situation that resulted, and wider lessons for the CS community.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1021–1022},
numpages = {2},
keywords = {comp.risks, experience report, learning from failure},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@InProceedings{sacnas-2022-curriculum-changes-for-a4a-poster,
  author =       {Jorge Nuñez and Dan Garcia and Jesus Villalobos and Destiny Luong and Jordan Schwartz and Jose Ochoa and Harjot Multani and Sebastian Estrada and Kamoni Fletcher},
  title =        {Curriculum Changes Necessary for Auto-grading and A’s for All},
  abstract = {A’s for All through mastery learning is the pedagogy we are pursuing in the classroom. “Fixed learning, and variable time” is our mantra. Our approach for achieving such methodology is through a computer-based testing (CBT) software called PrairieLearn: a free and open-source learning tool. This gives the instructor the ability to provide students with an ample supply of practice questions for a particular concept by authoring auto-graded question generators. The Beauty and Joy of Computing (CS10) is an introductory computer science course at UC Berkeley. Within this course, the first half is taught in a block-based computing language known as Snap! A huge development we are currently working on is creating an autograder that is compatible with Snap! in PrairieLearn. It will be a tool that will be of great help for our teaching model. Yet to fully implement this within our course, we have to reconsider how we structure our curriculum. A key component when creating a question generator has to pre-encode the answers to our questions. Meaning how we decide to ask certain questions will be changed because we can no longer have rubrics where an individual is involved. Snap! provides us with the ability to have human interactions within the code, although, now we are scraping those interactions. We are still providing students with fully-functional, rich, auto-graded questions, where the curriculum is automated centered. Still, according to our research, none of the important concepts are lost, and it will foster a curriculum that enables mastery learning.}, 
  booktitle = {The 2022 SACNAS National Diversity in STEM Conference},
  month =     10,
  location = {San Juan, PR, USA},
  year =      2022}

@InProceedings{sacnas-2022-holistic-approach-to-AG-in-a4a-learning-model-poster,
  author =       {Jesus Villalobos and Dan Garcia and Jorge Nuñez and Destiny Luong and Jordan Schwartz and Harjot Multani and Sebastian Estrada and Jose Ochoa and Kamoni Fletcher},
  title =        {A Holistic Approach to Effective Autograding in an "A's For All" Learning Model},
  abstract = {As learning models change, there is a departure from the “fixed time and variable learning” model to a “fixed learning and variable time” model known popularly as “A’s for All.” This model is characterized by consistent feedback and formative assessments instead of common summative exams with no feedback. By allowing variable pacing, students are able to learn until gaining proficiency without worrying about grades. They can expect to receive detailed feedback about their progress along with a pass to the next level, or supplementary materials and another opportunity to prove proficiency. At a small scale, this model requires minimal tooling but much energy and creativity. On the scale of large university courses, though, it is not sustainable to depend on staff readers to provide immediate feedback. In this investigation, we turn to a new software suite that dives into effective content creation, robust autograding, and the ability to provide immediate feedback. We begin with PrairieLearn (PL) – a platform that replaces single-instance questions with dynamic “variants” of a question. Along with a custom service that facilitates grading student work developed in the Snap! coding environment, we uphold the pillars of mastery learning: rich question availability and immediate feedback. Our developments are shaping a new way of assessing proficiency at scale. By reimagining what “grading” means while having autograded scores emulate historical trends, we will gauge our effectiveness. Our findings will serve as a roadmap to other large curriculums and courses, proving that “A’s for All” are both attainable and scalable.},
  booktitle = {The 2022 SACNAS National Diversity in STEM Conference},
  month =     10,
  location = {San Juan, PR, USA},
  year =      2022}

@InProceedings{tapia-2022-improving-equity-through-novel-assessments-and-grading-policies-panel,
  author =       {Dan Garcia and Fatima Alleyne and Armando Fox and Neal Terrell and Solomon Russell and Edwin Ambrosio},
  title =        {Improving Equity Through Novel Assessments and Grading Policies},
  abstract = {In early 2022, the Computer Science Education community was abuzz with several connected movements: “growth mindset”, “proficiency (aka mastery) learning”, “grading for equity”, and “specifications grading”. They are all steps toward the Holy Grail: “A’s for All (as time and interest allow)”; the “A” line doesn’t move, but every student is given an opportunity to achieve proficiency and earn it, as long as they are willing to put in the time and effort it might take. The mantra shifts from “fixed time, variable learning” to “fixed learning, variable time”. The foundational tenet is that all students can succeed in our institutions, but policies and structures have to be put in place for that to happen. Studies have shown that early success in introductory courses can have significant effects on belonging, and ramifications for career choices.

We’ll discuss many things that instructors can do to support students who have the intellectual capacity to learn the material, but just might need more time, possibly because their high school classes did not provide adequate preparation. One easy starting point is a switch from zero-sum curved grading to absolute grading. Scores on later cumulative exams can replace poor scores on an earlier exam. Projects can be configured to be autograded (with no hidden test cases), allowing their deadlines to soften, with no limit to the opportunities to achieve a perfect score. Exams can be authored in a system that allows for random question generation, allowing multiple chances to achieve proficiency.},
  booktitle = {CMD-IT/ACM Richard Tapia Celebration of Diversity in Computing Conference},
  month =     9,
  location = {Washington, DC, USA},
  year =      2022}
